{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# %install_ext https://raw.github.com/cpcloud/ipython-autotime/master/autotime.py\n",
    "%load_ext autotime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 504 ms\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./shooting_text_snippets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_content(df, col):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # tokenization\n",
    "    df['tokenized_words'] = df[col].apply(word_tokenize)\n",
    "    \n",
    "    # removing stop words\n",
    "    df['tokenized_stopped'] = df['tokenized_words'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    # POS tagging\n",
    "    df['tagged_stopped'] = df['tokenized_stopped'].apply(lambda x: nltk.pos_tag(x))\n",
    "    df['tagged'] = df['tokenized_words'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "    # Selecting adjectives\n",
    "    is_adj = lambda pos: pos[:2].__contains__('JJ')\n",
    "    df['adjectives'] = df['tagged_stopped'].apply(lambda x: [word for (word, pos) in x if is_adj(pos)])\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['lemmatized'] = df['adjectives'].apply(lambda x: [lemmatizer.lemmatize(item) for item in x])\n",
    "    \n",
    "    # Cleaning the result\n",
    "    df['lemmatized'].str.lower()\n",
    "    df['lemmatized'] = df['lemmatized'].apply(lambda x: [w for w in x if w.isalpha()])\n",
    "\n",
    "    return df\n",
    "\n",
    "df = process_content(df, 'snippet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('./df_new_0410.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.3 s\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./df_new_0410.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df_grouped = pd.DataFrame(df.groupby('ia_show_id')['lemmatized'].apply(lambda x: x.sum()))\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.FreqDist(df_grouped[''])     ### this can be used later\n",
    "# most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sent_score(word):\n",
    "    score = 0\n",
    "    a = TextBlob(word).sentiment\n",
    "    score += (a.polarity * a.subjectivity)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped['score'] = df_grouped['lemmatized'].apply(lambda x: [sent_score(item) for item in x])\n",
    "df_grouped['sentiment'] = df_grouped['score'].apply(lambda x: sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('./grouped_sentiment_0410.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textblob for whole sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped = pd.DataFrame(df.groupby('ia_show_id')['snippet'].apply(lambda x: x.sum()))\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_blob = []\n",
    "for i in range(df_grouped.shape[0]):\n",
    "    text_blob.append(TextBlob(df_grouped['snippet'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped['text_blob'] = text_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent = []\n",
    "for i in range(df_grouped.shape[0]):\n",
    "    sent.append(df_grouped['text_blob'][i].sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped['sentences'] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped['sentiment'] = df_grouped['text_blob'].apply(lambda x: x.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = []\n",
    "for i in range(df_grouped.shape[0]):\n",
    "    score.append(df_grouped['sentiment'][i].polarity * df_grouped['sentiment'][i].subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped['score'] = score\n",
    "df_grouped.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped.to_csv('./grouped_sentiment_0610.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./sentiments_with_race.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new = pd.concat([df_grouped[['ia_show_id', 'score']], df[['Race', 'Gender', 'Total victims']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df_new.copy(deep = True)\n",
    "df.rename(columns={'score':'sentiment'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "score_scaled = normalizer.fit_transform(df['sentiment'])\n",
    "df['score_norm'] = score_scaled[0]\n",
    "\n",
    "df['Gender'] = df['Gender'].str.replace('Male','M')\n",
    "# df['Injured'] = df['Injured'].replace({r\"[a-zA-Z]\", ''}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset X and y\n",
    "# X = df[['Fatalities', 'Injured', 'Total victims']]\n",
    "# one_hot = pd.get_dummies(df[['Venue', 'Race', 'Gender']])\n",
    "\n",
    "# X = pd.concat([X, one_hot], axis=1)\n",
    "X = pd.get_dummies(df[['Race', 'Gender']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df['score_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, random_state=0, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "rf = RandomForestRegressor(bootstrap=True, warm_start=True)\n",
    "estimator_range = range(1, 100, 5)\n",
    "for n_estimators in estimator_range:\n",
    "    rf.n_estimators = n_estimators\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_scores.append(rf.score(X_train, y_train))\n",
    "    test_scores.append(rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(estimator_range, test_scores, label=\"test scores\")\n",
    "plt.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.barh(range(rf.feature_importances_.shape[0]), rf.feature_importances_)\n",
    "plt.yticks(range(rf.feature_importances_.shape[0]), X.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor().fit(X_train, y_train)\n",
    "gbrt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [.5, .2, .1, .05, .02, .01, .001]}\n",
    "grid = GridSearchCV(GradientBoostingRegressor(), param_grid=param_grid, cv=10)\n",
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "scores.plot(x='param_learning_rate', y='mean_train_score', yerr='std_train_score', ax=plt.gca())\n",
    "scores.plot(x='param_learning_rate', y='mean_test_score', yerr='std_test_score', ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "fig, axs = plot_partial_dependence(gbrt, X_train, np.argsort(gbrt.feature_importances_)[-6:],\n",
    "                                       feature_names=X.columns,\n",
    "                                       n_jobs=3, grid_resolution=50)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary race white-non-white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Race'] = df['Race'].str.replace('White', '0')\n",
    "df['Race'] = df['Race'].str.replace('Native American', '1')\n",
    "df['Race'] = df['Race'].str.replace('Latino', '1')\n",
    "df['Race'] = df['Race'].str.replace('Other', '1')\n",
    "df['Race'] = df['Race'].str.replace('Black', '1')\n",
    "df['Race'] = df['Race'].str.replace('Asian', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df[['Race', 'Gender']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df['score_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, random_state=0, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adjective distrubtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 105 ms\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./sentiments_with_race.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5745711653373082"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.55 ms\n"
     ]
    }
   ],
   "source": [
    "df_white.sentiment.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 92.2 ms\n"
     ]
    }
   ],
   "source": [
    "df['lemmatized'] = df['lemmatized'].str.replace(\"'\",\"\")\n",
    "df['lemmatized'] = df['lemmatized'].str.replace(\"[\",\"\")\n",
    "df['lemmatized'] = df['lemmatized'].str.replace(\"]\",\"\")\n",
    "df['lemmatized'] = df['lemmatized'].str.replace(\" \",\"\")\n",
    "df['lemmatized'] = df['lemmatized'].str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11404"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 42.8 ms\n"
     ]
    }
   ],
   "source": [
    "all_words = [item for sublist in adj for item in sublist]\n",
    "len(all_words)\n",
    "len(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 44.6 ms\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.47 ms\n"
     ]
    }
   ],
   "source": [
    "df_white = df[df['Race'] == 'White']\n",
    "df_white.reset_index(inplace=True)\n",
    "df_other = df[df['Race'] != 'White']\n",
    "df_other.reset_index(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.87 ms\n"
     ]
    }
   ],
   "source": [
    "def adj_fred(df):\n",
    "    \n",
    "    words = []\n",
    "    for i in range(df.shape[0]):\n",
    "        words.append(df['lemmatized'][i])\n",
    "    w_list = [y for x in words for y in x]\n",
    "    return nltk.FreqDist(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.3 ms\n"
     ]
    }
   ],
   "source": [
    "white_list = adj_fred(df_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 91.3 ms\n"
     ]
    }
   ],
   "source": [
    "other_list = adj_fred(df_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.73 ms\n"
     ]
    }
   ],
   "source": [
    "white_key = list(white_list.keys())\n",
    "white_values = list(white_list.values())\n",
    "\n",
    "white = pd.DataFrame()\n",
    "white['adj'] = white_key\n",
    "white['freq'] = white_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.1 ms\n"
     ]
    }
   ],
   "source": [
    "other_key = list(other_list.keys())\n",
    "other_values = list(other_list.values())\n",
    "\n",
    "other = pd.DataFrame()\n",
    "other['adj'] = other_key\n",
    "other['freq'] = other_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 49.9 ms\n"
     ]
    }
   ],
   "source": [
    "other.to_csv('./other.csv')\n",
    "white.to_csv('./white.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
