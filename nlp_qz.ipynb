{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# %install_ext https://raw.github.com/cpcloud/ipython-autotime/master/autotime.py\n",
    "%load_ext autotime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 510 ms\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./shooting_text_snippets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_content(df, col):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # tokenization\n",
    "    df['tokenized_words'] = df[col].apply(word_tokenize)\n",
    "    \n",
    "    # removing stop words\n",
    "    df['tokenized_stopped'] = df['tokenized_words'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    # POS tagging\n",
    "    df['tagged_stopped'] = df['tokenized_stopped'].apply(lambda x: nltk.pos_tag(x))\n",
    "    df['tagged'] = df['tokenized_words'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "    # Selecting adjectives\n",
    "    is_adj = lambda pos: pos[:2].__contains__('JJ')\n",
    "    df['adjectives'] = df['tagged_stopped'].apply(lambda x: [word for (word, pos) in x if is_adj(pos)])\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['lemmatized'] = df['adjectives'].apply(lambda x: [lemmatizer.lemmatize(item) for item in x])\n",
    "    \n",
    "    # Cleaning the result\n",
    "    df['lemmatized'].str.lower()\n",
    "    df['lemmatized'] = df['lemmatized'].apply(lambda x: [w for w in x if w.isalpha()])\n",
    "\n",
    "    return df\n",
    "\n",
    "df = process_content(df, 'snippet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('./df_new_0410.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./df_new_0410.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df_grouped = pd.DataFrame(df.groupby('ia_show_id')['lemmatized'].apply(lambda x: x.sum()))\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.FreqDist(df_grouped[''])     ### this can be used later\n",
    "# most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sent_score(word):\n",
    "    score = 0\n",
    "    a = TextBlob(word).sentiment\n",
    "    score += (a.polarity * a.subjectivity)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped['score'] = df_grouped['lemmatized'].apply(lambda x: [sent_score(item) for item in x])\n",
    "df_grouped['sentiment'] = df_grouped['score'].apply(lambda x: sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('./grouped_sentiment_0410.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### textblob for whole sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snippet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ia_show_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BLOOMBERG_20140220_150000_Market_Makers</th>\n",
       "      <td>the population here in kiev is in a state of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLOOMBERG_20140220_180000_Bloomberg_West</th>\n",
       "      <td>the death toll has risen to 64. are bloomberg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLOOMBERG_20140221_130000_In_the_Loop_With_Betty_Liu</th>\n",
       "      <td>for gm, this is a huge market. really for all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLOOMBERG_20140221_200000_Street_Smart_with_Trish_Regan_and_Adam_Johnson</th>\n",
       "      <td>these elements all play into just civil unrest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLOOMBERG_20140403_050000_Countdown</th>\n",
       "      <td>there is a strong data. it is setting up. than...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              snippet\n",
       "ia_show_id                                                                                           \n",
       "BLOOMBERG_20140220_150000_Market_Makers             the population here in kiev is in a state of s...\n",
       "BLOOMBERG_20140220_180000_Bloomberg_West            the death toll has risen to 64. are bloomberg ...\n",
       "BLOOMBERG_20140221_130000_In_the_Loop_With_Bett...  for gm, this is a huge market. really for all ...\n",
       "BLOOMBERG_20140221_200000_Street_Smart_with_Tri...  these elements all play into just civil unrest...\n",
       "BLOOMBERG_20140403_050000_Countdown                 there is a strong data. it is setting up. than..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 626 ms\n"
     ]
    }
   ],
   "source": [
    "df_grouped = pd.DataFrame(df.groupby('ia_show_id')['snippet'].apply(lambda x: x.sum()))\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 416 ms\n"
     ]
    }
   ],
   "source": [
    "text_blob = []\n",
    "for i in range(df_grouped.shape[0]):\n",
    "    text_blob.append(TextBlob(df_grouped['snippet'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.73 ms\n"
     ]
    }
   ],
   "source": [
    "df_grouped['text_blob'] = text_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "sent = []\n",
    "for i in range(df_grouped.shape[0]):\n",
    "    sent.append(df_grouped['text_blob'][i].sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.22 ms\n"
     ]
    }
   ],
   "source": [
    "df_grouped['sentences'] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "df_grouped['sentiment'] = df_grouped['text_blob'].apply(lambda x: x.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 131 ms\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "for i in range(df_grouped.shape[0]):\n",
    "    score.append(df_grouped['sentiment'][i].polarity * df_grouped['sentiment'][i].subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.21 ms\n"
     ]
    }
   ],
   "source": [
    "df_grouped['score'] = score\n",
    "df_grouped.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_grouped.to_csv('./grouped_sentiment_0610.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 594 ms\n"
     ]
    }
   ],
   "source": [
    "df_grouped = pd.read_csv('./grouped_sentiment_0610.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "def process_content(df, col):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # tokenization\n",
    "    df['tokenized_words'] = df[col].apply(word_tokenize)\n",
    "    \n",
    "    # removing stop words\n",
    "#     df['tokenized_stopped'] = df['tokenized_words'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    # POS tagging\n",
    "#     df['tagged_stopped'] = df['tokenized_stopped'].apply(lambda x: nltk.pos_tag(x))\n",
    "    df['tagged'] = df['tokenized_words'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "#     df['namedEnt'] = df['tagged_stopped'].apply(lambda x: nltk.ne_chunk(x, binary=True))\n",
    "\n",
    "#     # Selecting adjectives\n",
    "#     is_adj = lambda pos: pos[:2].__contains__('JJ')\n",
    "#     df['adjectives'] = df['tagged_stopped'].apply(lambda x: [word for (word, pos) in x if is_adj(pos)])\n",
    "    \n",
    "#     # Lemmatization\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     df['lemmatized'] = df['adjectives'].apply(lambda x: [lemmatizer.lemmatize(item) for item in x])\n",
    "    \n",
    "#     # Cleaning the result\n",
    "#     df['lemmatized'].str.lower()\n",
    "#     df['lemmatized'] = df['lemmatized'].apply(lambda x: [w for w in x if w.isalpha()])\n",
    "\n",
    "    return df\n",
    "\n",
    "df_ = process_content(df_grouped, 'snippet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(the, DT), (population, NN), (here, RB), (in,...\n",
       "1    [(the, DT), (death, NN), (toll, NN), (has, VBZ...\n",
       "2    [(for, IN), (gm, NN), (,, ,), (this, DT), (is,...\n",
       "3    [(these, DT), (elements, NNS), (all, DT), (pla...\n",
       "4    [(there, EX), (is, VBZ), (a, DT), (strong, JJ)...\n",
       "Name: namedEnt, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19min 22s\n"
     ]
    }
   ],
   "source": [
    "df_['namedEnt'] = df_['tagged'].apply(lambda x: nltk.ne_chunk(x, binary=False))\n",
    "df_['namedEnt'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ia_show_id</th>\n",
       "      <th>snippet</th>\n",
       "      <th>text_blob</th>\n",
       "      <th>sentences</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>tokenized_stopped</th>\n",
       "      <th>tagged_stopped</th>\n",
       "      <th>namedEnt</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>BLOOMBERG_20140220_150000_Market_Makers</td>\n",
       "      <td>the population here in kiev is in a state of s...</td>\n",
       "      <td>the population here in kiev is in a state of s...</td>\n",
       "      <td>[Sentence(\"the population here in kiev is in a...</td>\n",
       "      <td>-0.061481</td>\n",
       "      <td>Sentiment(polarity=-0.13333333333333333, subje...</td>\n",
       "      <td>[the, population, here, in, kiev, is, in, a, s...</td>\n",
       "      <td>[population, kiev, state, shock, ., never, exp...</td>\n",
       "      <td>[(population, NN), (kiev, NN), (state, NN), (s...</td>\n",
       "      <td>[(the, DT), (population, NN), (here, RB), (in,...</td>\n",
       "      <td>[last, chaotic, population.i, coming, speak, j...</td>\n",
       "      <td>[(the, DT), (population, NN), (here, RB), (in,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>BLOOMBERG_20140220_180000_Bloomberg_West</td>\n",
       "      <td>the death toll has risen to 64. are bloomberg ...</td>\n",
       "      <td>the death toll has risen to 64. are bloomberg ...</td>\n",
       "      <td>[Sentence(\"the death toll has risen to 64. are...</td>\n",
       "      <td>0.058265</td>\n",
       "      <td>Sentiment(polarity=0.1202020202020202, subject...</td>\n",
       "      <td>[the, death, toll, has, risen, to, 64., are, b...</td>\n",
       "      <td>[death, toll, risen, 64., bloomberg, news, rep...</td>\n",
       "      <td>[(death, NN), (toll, NN), (risen, VBP), (64., ...</td>\n",
       "      <td>[(the, DT), (death, NN), (toll, NN), (has, VBZ...</td>\n",
       "      <td>[bloomberg, live, central, latest, nearby, squ...</td>\n",
       "      <td>[(the, DT), (death, NN), (toll, NN), (has, VBZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>BLOOMBERG_20140221_130000_In_the_Loop_With_Bet...</td>\n",
       "      <td>for gm, this is a huge market. really for all ...</td>\n",
       "      <td>for gm, this is a huge market. really for all ...</td>\n",
       "      <td>[Sentence(\"for gm, this is a huge market.\"), S...</td>\n",
       "      <td>0.073395</td>\n",
       "      <td>Sentiment(polarity=0.16111111111111112, subjec...</td>\n",
       "      <td>[for, gm, ,, this, is, a, huge, market, ., rea...</td>\n",
       "      <td>[gm, ,, huge, market, ., really, big, carmaker...</td>\n",
       "      <td>[(gm, NN), (,, ,), (huge, JJ), (market, NN), (...</td>\n",
       "      <td>[(for, IN), (gm, NN), (,, ,), (this, DT), (is,...</td>\n",
       "      <td>[huge, big, biggest, main]</td>\n",
       "      <td>[(for, IN), (gm, NN), (,, ,), (this, DT), (is,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>BLOOMBERG_20140221_200000_Street_Smart_with_Tr...</td>\n",
       "      <td>these elements all play into just civil unrest...</td>\n",
       "      <td>these elements all play into just civil unrest...</td>\n",
       "      <td>[Sentence(\"these elements all play into just c...</td>\n",
       "      <td>-0.076667</td>\n",
       "      <td>Sentiment(polarity=-0.10000000000000002, subje...</td>\n",
       "      <td>[these, elements, all, play, into, just, civil...</td>\n",
       "      <td>[elements, play, civil, unrest, taking, street...</td>\n",
       "      <td>[(elements, NNS), (play, VBP), (civil, JJ), (u...</td>\n",
       "      <td>[(these, DT), (elements, NNS), (all, DT), (pla...</td>\n",
       "      <td>[civil, best, angry, violent, national, tear, ...</td>\n",
       "      <td>[(these, DT), (elements, NNS), (all, DT), (pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>BLOOMBERG_20140403_050000_Countdown</td>\n",
       "      <td>there is a strong data. it is setting up. than...</td>\n",
       "      <td>there is a strong data. it is setting up. than...</td>\n",
       "      <td>[Sentence(\"there is a strong data.\"), Sentence...</td>\n",
       "      <td>0.016825</td>\n",
       "      <td>Sentiment(polarity=0.03333333333333334, subjec...</td>\n",
       "      <td>[there, is, a, strong, data, ., it, is, settin...</td>\n",
       "      <td>[strong, data, ., setting, ., thank, ., top, h...</td>\n",
       "      <td>[(strong, JJ), (data, NNS), (., .), (setting, ...</td>\n",
       "      <td>[(there, EX), (is, VBZ), (a, DT), (strong, JJ)...</td>\n",
       "      <td>[strong, top, top, national, texas]</td>\n",
       "      <td>[(there, EX), (is, VBZ), (a, DT), (strong, JJ)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         ia_show_id  \\\n",
       "0           0            BLOOMBERG_20140220_150000_Market_Makers   \n",
       "1           1           BLOOMBERG_20140220_180000_Bloomberg_West   \n",
       "2           2  BLOOMBERG_20140221_130000_In_the_Loop_With_Bet...   \n",
       "3           3  BLOOMBERG_20140221_200000_Street_Smart_with_Tr...   \n",
       "4           4                BLOOMBERG_20140403_050000_Countdown   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  the population here in kiev is in a state of s...   \n",
       "1  the death toll has risen to 64. are bloomberg ...   \n",
       "2  for gm, this is a huge market. really for all ...   \n",
       "3  these elements all play into just civil unrest...   \n",
       "4  there is a strong data. it is setting up. than...   \n",
       "\n",
       "                                           text_blob  \\\n",
       "0  the population here in kiev is in a state of s...   \n",
       "1  the death toll has risen to 64. are bloomberg ...   \n",
       "2  for gm, this is a huge market. really for all ...   \n",
       "3  these elements all play into just civil unrest...   \n",
       "4  there is a strong data. it is setting up. than...   \n",
       "\n",
       "                                           sentences     score  \\\n",
       "0  [Sentence(\"the population here in kiev is in a... -0.061481   \n",
       "1  [Sentence(\"the death toll has risen to 64. are...  0.058265   \n",
       "2  [Sentence(\"for gm, this is a huge market.\"), S...  0.073395   \n",
       "3  [Sentence(\"these elements all play into just c... -0.076667   \n",
       "4  [Sentence(\"there is a strong data.\"), Sentence...  0.016825   \n",
       "\n",
       "                                           sentiment  \\\n",
       "0  Sentiment(polarity=-0.13333333333333333, subje...   \n",
       "1  Sentiment(polarity=0.1202020202020202, subject...   \n",
       "2  Sentiment(polarity=0.16111111111111112, subjec...   \n",
       "3  Sentiment(polarity=-0.10000000000000002, subje...   \n",
       "4  Sentiment(polarity=0.03333333333333334, subjec...   \n",
       "\n",
       "                                     tokenized_words  \\\n",
       "0  [the, population, here, in, kiev, is, in, a, s...   \n",
       "1  [the, death, toll, has, risen, to, 64., are, b...   \n",
       "2  [for, gm, ,, this, is, a, huge, market, ., rea...   \n",
       "3  [these, elements, all, play, into, just, civil...   \n",
       "4  [there, is, a, strong, data, ., it, is, settin...   \n",
       "\n",
       "                                   tokenized_stopped  \\\n",
       "0  [population, kiev, state, shock, ., never, exp...   \n",
       "1  [death, toll, risen, 64., bloomberg, news, rep...   \n",
       "2  [gm, ,, huge, market, ., really, big, carmaker...   \n",
       "3  [elements, play, civil, unrest, taking, street...   \n",
       "4  [strong, data, ., setting, ., thank, ., top, h...   \n",
       "\n",
       "                                      tagged_stopped  \\\n",
       "0  [(population, NN), (kiev, NN), (state, NN), (s...   \n",
       "1  [(death, NN), (toll, NN), (risen, VBP), (64., ...   \n",
       "2  [(gm, NN), (,, ,), (huge, JJ), (market, NN), (...   \n",
       "3  [(elements, NNS), (play, VBP), (civil, JJ), (u...   \n",
       "4  [(strong, JJ), (data, NNS), (., .), (setting, ...   \n",
       "\n",
       "                                            namedEnt  \\\n",
       "0  [(the, DT), (population, NN), (here, RB), (in,...   \n",
       "1  [(the, DT), (death, NN), (toll, NN), (has, VBZ...   \n",
       "2  [(for, IN), (gm, NN), (,, ,), (this, DT), (is,...   \n",
       "3  [(these, DT), (elements, NNS), (all, DT), (pla...   \n",
       "4  [(there, EX), (is, VBZ), (a, DT), (strong, JJ)...   \n",
       "\n",
       "                                          adjectives  \\\n",
       "0  [last, chaotic, population.i, coming, speak, j...   \n",
       "1  [bloomberg, live, central, latest, nearby, squ...   \n",
       "2                         [huge, big, biggest, main]   \n",
       "3  [civil, best, angry, violent, national, tear, ...   \n",
       "4                [strong, top, top, national, texas]   \n",
       "\n",
       "                                              tagged  \n",
       "0  [(the, DT), (population, NN), (here, RB), (in,...  \n",
       "1  [(the, DT), (death, NN), (toll, NN), (has, VBZ...  \n",
       "2  [(for, IN), (gm, NN), (,, ,), (this, DT), (is,...  \n",
       "3  [(these, DT), (elements, NNS), (all, DT), (pla...  \n",
       "4  [(there, EX), (is, VBZ), (a, DT), (strong, JJ)...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 263 ms\n"
     ]
    }
   ],
   "source": [
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# is_adj = lambda pos: pos[:2].__contains__('JJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# - remove location, then do adjective frequecny\n",
    "# - classificion with sentence-level sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./sentiments_with_race.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_new = pd.concat([df_grouped[['ia_show_id', 'score']], df[['Race', 'Gender', 'Total victims']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df_new.copy(deep = True)\n",
    "df.rename(columns={'score':'sentiment'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "score_scaled = normalizer.fit_transform(df['sentiment'])\n",
    "df['score_norm'] = score_scaled[0]\n",
    "\n",
    "df['Gender'] = df['Gender'].str.replace('Male','M')\n",
    "# df['Injured'] = df['Injured'].replace({r\"[a-zA-Z]\", ''}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset X and y\n",
    "# X = df[['Fatalities', 'Injured', 'Total victims']]\n",
    "# one_hot = pd.get_dummies(df[['Venue', 'Race', 'Gender']])\n",
    "\n",
    "# X = pd.concat([X, one_hot], axis=1)\n",
    "X = pd.get_dummies(df[['Race', 'Gender']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df['score_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, random_state=0, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "rf = RandomForestRegressor(bootstrap=True, warm_start=True)\n",
    "estimator_range = range(1, 100, 5)\n",
    "for n_estimators in estimator_range:\n",
    "    rf.n_estimators = n_estimators\n",
    "    rf.fit(X_train, y_train)\n",
    "    train_scores.append(rf.score(X_train, y_train))\n",
    "    test_scores.append(rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(estimator_range, test_scores, label=\"test scores\")\n",
    "plt.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.barh(range(rf.feature_importances_.shape[0]), rf.feature_importances_)\n",
    "plt.yticks(range(rf.feature_importances_.shape[0]), X.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor().fit(X_train, y_train)\n",
    "gbrt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [.5, .2, .1, .05, .02, .01, .001]}\n",
    "grid = GridSearchCV(GradientBoostingRegressor(), param_grid=param_grid, cv=10)\n",
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "scores.plot(x='param_learning_rate', y='mean_train_score', yerr='std_train_score', ax=plt.gca())\n",
    "scores.plot(x='param_learning_rate', y='mean_test_score', yerr='std_test_score', ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "fig, axs = plot_partial_dependence(gbrt, X_train, np.argsort(gbrt.feature_importances_)[-6:],\n",
    "                                       feature_names=X.columns,\n",
    "                                       n_jobs=3, grid_resolution=50)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary race white-non-white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Race'] = df['Race'].str.replace('White', '0')\n",
    "df['Race'] = df['Race'].str.replace('Native American', '1')\n",
    "df['Race'] = df['Race'].str.replace('Latino', '1')\n",
    "df['Race'] = df['Race'].str.replace('Other', '1')\n",
    "df['Race'] = df['Race'].str.replace('Black', '1')\n",
    "df['Race'] = df['Race'].str.replace('Asian', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df[['Race', 'Gender']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df['score_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, random_state=0, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adjective distrubtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./sentiments_with_race.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_white.sentiment.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['lemmatized'] = df['lemmatized'].str.replace(\"'\",\"\")\n",
    "df['lemmatized'] = df['lemmatized'].str.replace(\"[\",\"\")\n",
    "df['lemmatized'] = df['lemmatized'].str.replace(\"]\",\"\")\n",
    "df['lemmatized'] = df['lemmatized'].str.replace(\" \",\"\")\n",
    "df['lemmatized'] = df['lemmatized'].str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words = [item for sublist in adj for item in sublist]\n",
    "len(all_words)\n",
    "len(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_white = df[df['Race'] == 'White']\n",
    "df_white.reset_index(inplace=True)\n",
    "df_other = df[df['Race'] != 'White']\n",
    "df_other.reset_index(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adj_fred(df):\n",
    "    \n",
    "    words = []\n",
    "    for i in range(df.shape[0]):\n",
    "        words.append(df['lemmatized'][i])\n",
    "    w_list = [y for x in words for y in x]\n",
    "    return nltk.FreqDist(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "white_list = adj_fred(df_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "other_list = adj_fred(df_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "white_key = list(white_list.keys())\n",
    "white_values = list(white_list.values())\n",
    "\n",
    "white = pd.DataFrame()\n",
    "white['adj'] = white_key\n",
    "white['freq'] = white_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "other_key = list(other_list.keys())\n",
    "other_values = list(other_list.values())\n",
    "\n",
    "other = pd.DataFrame()\n",
    "other['adj'] = other_key\n",
    "other['freq'] = other_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "other.to_csv('./other.csv')\n",
    "white.to_csv('./white.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
